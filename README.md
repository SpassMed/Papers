# Papers
A list of all the relevant papers along with their codes.

# 2024:
Mar 12: **Chronos**: Learning the Language of Time Series- Amazon- [Paper](https://arxiv.org/abs/2403.07815v1) [Official Code](https://github.com/amazon-science/chronos-forecasting)

Mar 09: **S2IP-LLM**: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting- UConnecticut, Morgan Stanley- [Paper](https://arxiv.org/abs/2403.05798) 

Feb 29: **UniTS**: Building a Unified Time Series Model- Harvard, MIT- [Paper](https://arxiv.org/abs/2403.00131)   [Official Code](https://github.com/mims-harvard/UniTS)   [Our Code](https://github.com/SpassMed/UniTS)

Feb 29: **TimeXer**: Empowering Transformers for Time Series Forecasting with Exogenous Variables- Haixu Wu, Tsinghua University- [Paper](https://arxiv.org/abs/2402.19072v1)

Feb 06: **MOMENT**: A Family of Open Time-series Foundation Models- CMU- [Paper](https://arxiv.org/abs/2402.03885) [Official Code](https://anonymous.4open.science/r/BETT-773F)

Feb 05: **Survey Paper**: Empowering Time Series Analysis with Large Language Models- Morgan Stanley- [Paper](https://arxiv.org/abs/2402.03182)

Feb 04: **Pathformer**: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting- **ICLR 2024**- Alibaba- [Paper](https://arxiv.org/abs/2402.05956) [Official Code](https://github.com/decisionintelligence/pathformer)

Feb 04: **MOIRAI**: Unified Training of Universal Time Series Forecasting Transformers- SalesForce- [Paper](https://arxiv.org/abs/2402.02592)


Jan 17: **RWKV-TS**: Beyond Traditional Recurrent Neural Network for Time Series Tasks: Applying RWKV on time-series data- [Paper](https://arxiv.org/abs/2401.09093) [Official Code](https://github.com/howard-hou/RWKV-TS)



# 2023:
Nov 30: **MultiResFormer**: Transformer with Adaptive Multi-Resolution Modeling for General Time Series Forecasting- Layer6, Vector- [Paper](https://arxiv.org/abs/2311.18780v2)

Oct 12: **Lag-Llama**: Towards Foundation Models for Probabilistic Time Series Forecasting- Morgan Stanley, Mila, UMontreal, McGill-  [Paper](https://arxiv.org/abs/2310.08278)  [Official Code](https://github.com/time-series-foundation-models/lag-llama)

Oct 14: **TimesFM**: A decoder-only foundation model for time-series forecasting- Google-  [Paper](https://arxiv.org/abs/2310.10688)  [Official Blog](https://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html)

Oct 10: **iTransformer**: Inverted Transformers Are Effective for Time Series Forecasting, **ICLR 2024**- Haixu Wu, Tsinghua University, Ant Group- [Paper](https://arxiv.org/abs/2310.06625)  [Official Code](https://github.com/thuml/iTransformer)

Oct 05: **TimeGPT-1**: Nixtla-  [Paper](https://arxiv.org/abs/2310.03589)  [Official API](https://github.com/Nixtla/nixtla)

May 20: **CARD**: Channel Aligned Robust Blend Transformer for Time Series Forecasting- **ICLR 2024**- Alibaba- [Paper](https://arxiv.org/abs/2305.12095) [Official Code](https://github.com/wxie9/CARD)

Apr 23: **TimesNet**: Temporal 2D-Variation Modeling for General Time Series Analysis- Haixu Wu, Tsinghua University- **ICLR 2023**- [Paper](https://arxiv.org/abs/2210.02186) [Official Code](https://github.com/thuml/TimesNet)

Mar 10: **TSMixer**: An All-MLP Architecture for Time Series Forecasting- **TMLR 2023**- Google Cloud- [Paper](https://arxiv.org/abs/2303.06053) [Official Code](https://github.com/google-research/google-research/tree/master/tsmixer) [Time Series Library](https://github.com/thuml/Time-Series-Library/tree/main/models)

Mar 05: **PatchTST: A Time Series is Worth 64 Words**: Long-term Forecasting with Transformers- **ICLR 2023**-  [Paper](https://arxiv.org/abs/2211.14730)  [Official Code](https://github.com/yuqinie98/PatchTST)

Mar 01: **Time Series as Images**: Vision Transformer for Irregularly Sampled Time Series- **NeuriPS 2023**- [Paper](https://arxiv.org/abs/2303.12799) [Code](https://github.com/Leezekun/ViTST)



# 2022:
Jan 07: **Autoformer**: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting- **NeurIPS 2021**- Haixu Wu, Tsinghua University- [Paper](https://arxiv.org/abs/2106.13008) [Official Code](https://github.com/thuml/Autoformer)


# 2021:
Dec 19: **Masked Autoencoders** Are Scalable Vision Learners- [Paper](https://arxiv.org/abs/2111.06377) [Official Code](https://github.com/facebookresearch/mae)

Jan 27: **Res2Net**: A New Multi-scale Backbone Architecture- [Paper](https://arxiv.org/abs/1904.01169) [Official Code](https://github.com/Res2Net)

# 2019:
Dec 19: **TFT**: Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting- [Paper](https://arxiv.org/abs/1912.09363) [Official Code](https://github.com/google-research/google-research/tree/master/tft)
